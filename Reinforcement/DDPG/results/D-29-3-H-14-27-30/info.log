INFO:Reinforcement.Functions:results:[{'score': [], 'loss': []}]
INFO:Reinforcement.Functions:settings:[{'minGameSequence': 500, 'gamma': 0.95, 'dequeSize': 50000, 'learningRate': 0.001, 'minGameScore': 2736, 'minGameScoreRatio': 0.95, 'nGamesPerSave': 10, 'trainSetSize': 64, 'nModelBackups': 3, 'batchSize': 64, 'nEpochs': 1, 'TAU': 0.001, 'gameMinutesLength': 2880}]
INFO:Reinforcement.Functions:policy:[{'numOfDevices': 5, 'stateDim': (1, 8), 'seqLen': 1, 'fname': '/home/yochaiz/SmartHome/Reinforcement/Policies/Week/policy2.json'}]
INFO:Reinforcement.Functions:Actor:[{'nBackups': 3, 'TAU': 0.001, 'epsilon_min': 0.01, 'nActions': 32, 'epsilon': 1.0, 'epsilon_decay': 0.99, 'k': 32, 'actionDim': 5, 'curBackupIdx': 0}]
INFO:Reinforcement.Functions:args:[{'k': 32, 'gpuNum': 1, 'settings': '/home/yochaiz/SmartHome/Reinforcement/settings.json', 'random': True, 'sequential': False, 'desc': None, 'gpuFrac': 0.3}]
INFO:Reinforcement.Functions:Critic:[{'nBackups': 3, 'actionDim': 5, 'TAU': 0.001, 'curBackupIdx': 0}]
INFO:Reinforcement.Functions:Saving [Actor] main model as [/home/yochaiz/SmartHome/Reinforcement/DDPG/results/D-29-3-H-14-27-30/Actor-main-model-0.h5]
INFO:Reinforcement.Functions:Saving [Actor] target model as [/home/yochaiz/SmartHome/Reinforcement/DDPG/results/D-29-3-H-14-27-30/Actor-target-model-0.h5]
INFO:Reinforcement.Functions:Saving [Critic] main model as [/home/yochaiz/SmartHome/Reinforcement/DDPG/results/D-29-3-H-14-27-30/Critic-main-model-0.h5]
INFO:Reinforcement.Functions:Saving [Critic] target model as [/home/yochaiz/SmartHome/Reinforcement/DDPG/results/D-29-3-H-14-27-30/Critic-target-model-0.h5]
INFO:Reinforcement.Functions:===== DESCRIPTIONS =====
INFO:Reinforcement.Functions:[General]: None
INFO:Reinforcement.Functions:[Actor]: Standard (paper) architecture
INFO:Reinforcement.Functions:[Actor]: Fixed bug where future reward (step 13) used continuous action instead of discrete action
INFO:Reinforcement.Functions:[Actor]: No more threading during replays
INFO:Reinforcement.Functions:[Critic]: Standard (paper) architecture
INFO:Reinforcement.Functions:===== ============ =====
INFO:Reinforcement.Functions:[Actor] model architecture:
INFO:Reinforcement.Functions:_________________________________________________________________
INFO:Reinforcement.Functions:Layer (type)                 Output Shape              Param #   
INFO:Reinforcement.Functions:=================================================================
INFO:Reinforcement.Functions:input_1 (InputLayer)         (None, 1, 8)              0         
INFO:Reinforcement.Functions:_________________________________________________________________
INFO:Reinforcement.Functions:dense_1 (Dense)              (None, 1, 128)            1152      
INFO:Reinforcement.Functions:_________________________________________________________________
INFO:Reinforcement.Functions:dense_2 (Dense)              (None, 1, 64)             8256      
INFO:Reinforcement.Functions:_________________________________________________________________
INFO:Reinforcement.Functions:dense_3 (Dense)              (None, 1, 5)              325       
INFO:Reinforcement.Functions:_________________________________________________________________
INFO:Reinforcement.Functions:reshape_1 (Reshape)          (None, 5)                 0         
INFO:Reinforcement.Functions:=================================================================
INFO:Reinforcement.Functions:Total params: 9,733
INFO:Reinforcement.Functions:Trainable params: 9,733
INFO:Reinforcement.Functions:Non-trainable params: 0
INFO:Reinforcement.Functions:_________________________________________________________________
INFO:Reinforcement.Functions:[Critic] model architecture:
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:Layer (type)                    Output Shape         Param #     Connected to                     
INFO:Reinforcement.Functions:==================================================================================================
INFO:Reinforcement.Functions:input_2 (InputLayer)            (None, 1, 8)         0                                            
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:dense_4 (Dense)                 (None, 1, 128)       1152        input_2[0][0]                    
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:input_3 (InputLayer)            (None, 5)            0                                            
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:dense_5 (Dense)                 (None, 1, 64)        8256        dense_4[0][0]                    
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:dense_6 (Dense)                 (None, 64)           384         input_3[0][0]                    
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:add_1 (Add)                     (None, 1, 64)        0           dense_5[0][0]                    
INFO:Reinforcement.Functions:                                                                 dense_6[0][0]                    
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:activation_1 (Activation)       (None, 1, 64)        0           add_1[0][0]                      
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:dense_7 (Dense)                 (None, 1, 1)         65          activation_1[0][0]               
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:reshape_2 (Reshape)             (None, 1)            0           dense_7[0][0]                    
INFO:Reinforcement.Functions:==================================================================================================
INFO:Reinforcement.Functions:Total params: 9,857
INFO:Reinforcement.Functions:Trainable params: 9,857
INFO:Reinforcement.Functions:Non-trainable params: 0
INFO:Reinforcement.Functions:__________________________________________________________________________________________________
INFO:Reinforcement.Functions:episode: 1, score:[2178.40], loss:[156.96871], sequence:[0], isInPoolRatio:[1.00], optActionSelectedRatio:[0.63], optActionInPoolButNotSelected:[0.34], random actions:[114], eInit:[1.0000], init state:[ 0 13 15  0  0  0  0  0], end state:[ 2 13 15  0  0  0  0  0], runtime(seconds):[311.81]
INFO:Reinforcement.Functions:episode: 2, score:[1312.80], loss:[133.76176], sequence:[0], isInPoolRatio:[1.00], optActionSelectedRatio:[0.17], optActionInPoolButNotSelected:[0.82], random actions:[135], eInit:[0.9900], init state:[4 8 7 0 0 0 0 0], end state:[6 8 7 0 0 0 0 0], runtime(seconds):[312.53]
INFO:Reinforcement.Functions:episode: 3, score:[1655.60], loss:[106.02451], sequence:[0], isInPoolRatio:[1.00], optActionSelectedRatio:[0.38], optActionInPoolButNotSelected:[0.61], random actions:[111], eInit:[0.9801], init state:[ 3 23 59  1  0  0  0  0], end state:[ 5 23 59  0  1  0  0  0], runtime(seconds):[311.46]
